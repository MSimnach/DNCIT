{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e523a8-ee61-43b3-8195-2a0521b71363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "37f49b54-68b8-4730-869b-e0c7918bd8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "efd32887-5f91-452c-8759-e86ec6313e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8de8b99e-afde-47d9-a1c0-0f16d0727fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_to_rgb at 0x000001E917AA0180>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a0907b67-9735-430a-8e67-e26d06cf9945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "00a27cf7-a8ca-42d4-a4dc-86ee963acd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedMNIST v3.0.1 @ https://github.com/MedMNIST/MedMNIST/\n"
     ]
    }
   ],
   "source": [
    "print(f\"MedMNIST v{medmnist.__version__} @ {medmnist.HOMEPAGE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "669a01ed-b8d2-4c57-a466-6eff882aeae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag = 'breastmnist'\n",
    "# data_flag = 'breastmnist'\n",
    "download = True\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a86c681a-4293-4c62-9187-1117d930fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5bc1d159-887a-41fc-a947-660383b21fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\Marco\\.medmnist\\breastmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Marco\\.medmnist\\breastmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Marco\\.medmnist\\breastmnist.npz\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "train_dataset = DataClass(split='train', transform=preprocess, download=download, as_rgb=True)\n",
    "test_dataset = DataClass(split='test', transform=preprocess, download=download, as_rgb=False)\n",
    "\n",
    "pil_dataset = DataClass(split='train', download=download)\n",
    "\n",
    "start_index = 0\n",
    "end_index = 128  # Load first 128 samples, for example\n",
    "\n",
    "# Create a Subset of the dataset containing only the specified indices\n",
    "subset = torch.utils.data.Subset(train_dataset, range(start_index, end_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "74196b54-d388-4bae-a285-f938fe61ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(dataset=subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "pil_loader = data.DataLoader(dataset=pil_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "827cac25-903f-4b5e-9e5a-35d9721848d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x000001E94D035F40>\n"
     ]
    }
   ],
   "source": [
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "999c3458-89e3-41fa-9623-7120c35d48a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AMebUVgbyfMuZH7qY3TJ+rAVn3N/IbkK0ptFZco0mCW/A1PYxC4wq6qgYn5lZUb8sCrX9jSd9ciX0Hkiq954i3WLRkMwweSefzrlZ1W9eOWRizdOR2qxHZqM+WSCB1x0q20RkwzSOTjFcqL2TyyGJOexNaFhKoj3uFI7Bq1Bc7lICxIh6kKwpEvLdV273+XjqorkFJqW3djIATxnpXTW8IltlUs4VhggNxUS2MMe5QGIz3Nf/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACN0lEQVR4AR1SSY4cNxDMTGaRrK7unm5IkDAXHeSD3uDv+Df+oaGLIEEYYSAMequVzMW0r4mIyIhA4N9Tip08TGtR92R3KNwLQOkV+J9MiqFgjAsQzBGN8rsJ4W4lcsEhkDCLenmRUSthTjWmWTbkgVSZ48FWmfbC/pQbyQJbTcbDkKexe/v+4fZD9x8+3R74/Jlfn57WSj3/sW7r4/fNv61YKHWJ9Pocn4fDzDTgn/UKu/d0HHfnyyWfMaPj+aAZg1f+6+sDXufj7nQKv15L2kXBrj+czzXhjccvtn65Tr/r+Pb9ClulQHS7XPpDf3/wr5fTR3lM82xOvVtC7FoyecPVCm+w3G5jwPXR9QC2I0ueWMqYUVly+em8tO5CNkXu4qx71+S6D7wKmMj9svS9g7UwcAR3y7lPE48j26bb0jCpMZsAeJef6vqYiCch8+CqFVQMOwSyaupdZOPFEX1ZauveMQZkW8Jw0rkBkCezvRSrFamZlapgPS3bXU4B+KqwkjsSWtd1AwVNiVZl98ZcDJwRDMWUQXbHaHXUKNsWKxfqakNpYCIKMF9VzTNQ9j3z1nnXVtAGQzavTSWC+hSPQ8DCTbC21KIC+9z955i8pgjXFzsyobevhiGmVoUVB4hhZyvtYuTmn9uBCJojaQUz7r0I+r1Nq/G0BQkmi5I6y3DKwHWahTODt6WCN0tSqFUE8xpCLGJu/0uCAbk5oBNyyqxOfMhCJzbyNilHBWwSWhDTIMQoW+3+BUuQhkJSzF5RAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.montage(length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9cba7915-4204-4c22-885a-38a16a8a1faa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List to store feature representations\n",
    "features = []\n",
    "\n",
    "# Iterate over the DataLoader to access batches of data\n",
    "for images, _ in train_loader:\n",
    "    # Apply transformations to the images\n",
    "    # images = preprocess(images).to(device)\n",
    "\n",
    "    # Obtain embeddings for the images\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode_image(images).float()\n",
    "        embeddings /= embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Append the embeddings to the features list\n",
    "    features.append(embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b85d69f2-c01b-40aa-9434-6431150fa2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0392431  -0.07876793 -0.02355219 ...  0.02742301 -0.00583899\n",
      "  -0.02134407]\n",
      " [-0.04859295 -0.05071741  0.01970769 ...  0.02237301  0.01924399\n",
      "  -0.02781691]\n",
      " [-0.04988016 -0.10676885 -0.01520137 ...  0.04452096 -0.00512853\n",
      "  -0.03463099]\n",
      " ...\n",
      " [-0.04873145  0.01847215  0.01132574 ...  0.00362399  0.01225031\n",
      "  -0.02893541]\n",
      " [-0.04174479 -0.0778585   0.01112214 ...  0.00762837  0.00323934\n",
      "  -0.05025019]\n",
      " [-0.01384686  0.00228164 -0.01217282 ...  0.01448514 -0.01558171\n",
      "  -0.02966568]]\n",
      "(64, 512)\n",
      "2\n",
      "[array([[-0.0392431 , -0.07876793, -0.02355219, ...,  0.02742301,\n",
      "        -0.00583899, -0.02134407],\n",
      "       [-0.04859295, -0.05071741,  0.01970769, ...,  0.02237301,\n",
      "         0.01924399, -0.02781691],\n",
      "       [-0.04988016, -0.10676885, -0.01520137, ...,  0.04452096,\n",
      "        -0.00512853, -0.03463099],\n",
      "       ...,\n",
      "       [-0.04873145,  0.01847215,  0.01132574, ...,  0.00362399,\n",
      "         0.01225031, -0.02893541],\n",
      "       [-0.04174479, -0.0778585 ,  0.01112214, ...,  0.00762837,\n",
      "         0.00323934, -0.05025019],\n",
      "       [-0.01384686,  0.00228164, -0.01217282, ...,  0.01448514,\n",
      "        -0.01558171, -0.02966568]], dtype=float32), array([[-0.04972139,  0.02154206,  0.00850335, ...,  0.03209221,\n",
      "        -0.00580077, -0.05485661],\n",
      "       [-0.01493995, -0.0519909 , -0.03157634, ...,  0.0198574 ,\n",
      "         0.01129281, -0.00485298],\n",
      "       [-0.04660459,  0.02026975,  0.02026554, ...,  0.02789307,\n",
      "         0.01574162, -0.04430561],\n",
      "       ...,\n",
      "       [-0.02185875, -0.1032257 , -0.02253054, ...,  0.01676201,\n",
      "         0.00157306,  0.00622132],\n",
      "       [-0.02722801, -0.02725173, -0.02918559, ...,  0.00397793,\n",
      "         0.02876997, -0.03975576],\n",
      "       [-0.030844  ,  0.01359671, -0.01109132, ...,  0.03622404,\n",
      "         0.01187694, -0.01836169]], dtype=float32)]\n",
      "(128, 512)\n"
     ]
    }
   ],
   "source": [
    "print(features[0])\n",
    "print(features[0].shape)\n",
    "print(len(features))\n",
    "print(features)\n",
    "print(np.concatenate(features, axis=0).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
